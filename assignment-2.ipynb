{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sentiment as sentimentinterface\n",
    "import classify \n",
    "import timeit\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data\n",
      "-- train data\n",
      "sentiment/train.tsv\n",
      "4582\n",
      "-- dev data\n",
      "sentiment/dev.tsv\n",
      "458\n",
      "-- transforming data and labels\n",
      "  Accuracy on train  is: 0.898515931907464\n",
      "  Accuracy on dev  is: 0.7794759825327511\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data\")\n",
    "tarfname = \"sentiment.tar.gz\"\n",
    "sentiment = sentimentinterface.read_files(tarfname)\n",
    "vectorizer = TfidfVectorizer(stop_words=['and','a','the','this','that','an','there','here','those','am','it','me','with'],ngram_range=(1, 2),max_df=1000, min_df=5)\n",
    "training_features = vectorizer.fit_transform(sentiment.train_data)  \n",
    "dev_features = vectorizer.transform(sentiment.dev_data)\n",
    "cls = classify.train_classifier(training_features, sentiment.trainy)\n",
    "classify.evaluate(training_features, sentiment.trainy, cls, 'train')\n",
    "classify.evaluate(dev_features, sentiment.devy, cls, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy on train  is: 0.9821038847664775\n",
      "  Accuracy on dev  is: 0.777292576419214\n"
     ]
    }
   ],
   "source": [
    "cls1 = classify.train_classifier(sentiment.trainX, sentiment.trainy)\n",
    "classify.evaluate(sentiment.trainX, sentiment.trainy, cls1, 'train')\n",
    "classify.evaluate(sentiment.devX, sentiment.devy, cls1, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment/unlabeled.tsv\n",
      "(91524, 4478)\n"
     ]
    }
   ],
   "source": [
    "def read_unlabeled(tarfname, sentiment,vectorizer):\n",
    "    \"\"\"Reads the unlabeled data.\n",
    "\n",
    "    The returned object contains three fields that represent the unlabeled data.\n",
    "\n",
    "    data: documents, represented as sequence of words\n",
    "    fnames: list of filenames, one for each document\n",
    "    X: bag of word vector for each document, using the sentiment.vectorizer\n",
    "    \"\"\"\n",
    "    import tarfile\n",
    "    tar = tarfile.open(tarfname, \"r:gz\")\n",
    "    class Data: pass\n",
    "    unlabeled = Data()\n",
    "    unlabeled.data = []\n",
    "    \n",
    "    unlabeledname = \"unlabeled.tsv\"\n",
    "    for member in tar.getmembers():\n",
    "        if 'unlabeled.tsv' in member.name:\n",
    "            unlabeledname = member.name\n",
    "            \n",
    "    print(unlabeledname)\n",
    "    tf = tar.extractfile(unlabeledname)\n",
    "    for line in tf:\n",
    "        line = line.decode(\"utf-8\")\n",
    "        text = line.strip()\n",
    "        unlabeled.data.append(text)        \n",
    "    unlabeled.X = vectorizer.transform(unlabeled.data)\n",
    "    print(unlabeled.X.shape)\n",
    "    tar.close()\n",
    "    return unlabeled\n",
    "unlabeled = read_unlabeled(tarfname, sentiment,vectorizer)\n",
    "#sentimentinterface.write_pred_kaggle_file(unlabeled, cls_quarter, \"trial\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_quarter=cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_pred=cls_quarter.predict(unlabeled.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob=cls_quarter.predict_proba(unlabeled.X)\n",
    "p=np.where(prob[:,0] > 0.95)\n",
    "q=np.where(prob[:,0] < 0.5)\n",
    "r=np.concatenate((p,q),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy on dev  is: 0.5611353711790393\n"
     ]
    }
   ],
   "source": [
    "new_unlab=np.array(unlabeled.data)\n",
    "new_un_pred=np.array(unlabeled_pred)\n",
    "train_new=np.array(list(sentiment.train_data)+list(new_unlab[r]))\n",
    "trainy_new=np.array(list(sentiment.trainy)+list(new_un_pred[r]))\n",
    "training_features_q = vectorizer.fit_transform(train_new)  \n",
    "dev_features = vectorizer.transform(sentiment.dev_data)\n",
    "cls_quarter=classify.train_classifier(training_features_q, trainy_new)\n",
    "classify.evaluate(dev_features, sentiment.devy, cls_quarter, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Top k=8\n",
      "--------------------------------------------------\n",
      "friendly\n",
      "awesome\n",
      "excellent\n",
      "delicious\n",
      "amazing\n",
      "love\n",
      "best\n",
      "great\n",
      "--------------------------------------------------\n",
      "Bottom k=8\n",
      "--------------------------------------------------\n",
      "not\n",
      "worst\n",
      "horrible\n",
      "rude\n",
      "terrible\n",
      "bad\n",
      "went\n",
      "star\n"
     ]
    }
   ],
   "source": [
    "coefficients=cls_quarter.coef_[0]\n",
    "k = 8\n",
    "top_k =np.argsort(coefficients)[-k:]\n",
    "top_k_words = []\n",
    "\n",
    "print('-'*50)\n",
    "print('Top k=%d' %k)\n",
    "print('-'*50)\n",
    "\n",
    "for i in top_k:\n",
    "    print(vectorizer.get_feature_names()[i])\n",
    "    top_k_words.append(vectorizer.get_feature_names()[i])\n",
    "#print(sentiment.count_ve\n",
    "print('-'*50)\n",
    "print('Bottom k=%d' %k)\n",
    "print('-'*50)\n",
    "#top_k = np.argpartition(coefficients, -k)[-k:]\n",
    "bottom_k =np.argsort(coefficients)[:k]\n",
    "bottom_k_words = []\n",
    "#print(top_k)\n",
    "for i in bottom_k:\n",
    "    print(vectorizer.get_feature_names()[i])\n",
    "    bottom_k_words.append(vectorizer.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=3,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "import string\n",
    "punc = string.punctuation\n",
    "for i in sentiment.train_data:\n",
    "    thestring = i\n",
    "    s = list(thestring)\n",
    "    train_data.append(''.join([o for o in s if not o in punc]).lower().split())\n",
    "dev_data=[]\n",
    "for i in sentiment.dev_data:\n",
    "    thestring = i\n",
    "    s = list(thestring)\n",
    "    dev_data.append(''.join([o for o in s if not o in punc]).lower().split())\n",
    "test_data=[]\n",
    "for i in unlabeled.data:\n",
    "    thestring = i\n",
    "    s = list(thestring)\n",
    "    test_data.append(''.join([o for o in s if not o in punc]).lower().split())\n",
    "total_data= train_data+dev_data+test_data \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100020871, 272021500)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.build_vocab(total_data, progress_per=10000)\n",
    "w2v_model.train(train_data+dev_data+test_data, total_examples=w2v_model.corpus_count, epochs=100, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 0.7881866097450256),\n",
       " ('decent', 0.6825694441795349),\n",
       " ('but', 0.6501556634902954),\n",
       " ('delicious', 0.6254854798316956),\n",
       " ('excellent', 0.6209093332290649),\n",
       " ('really', 0.6112504005432129),\n",
       " ('food', 0.6086443662643433),\n",
       " ('tasty', 0.6044114828109741),\n",
       " ('nice', 0.5849175453186035),\n",
       " ('amazing', 0.582426905632019)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(w2v_model, open('word2vec', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = pickle.load(open('word2vec', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "train_wv=[]\n",
    "for i in train_data:\n",
    "    temp=0;count=0\n",
    "    for j in i:\n",
    "        if j in w2v_model.wv.vocab and len(i)!=0:\n",
    "            temp+=w2v_model[j]\n",
    "            count+=1\n",
    "    if len(i)!=0:\n",
    "        train_wv.append((temp/count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy on train  is: 0.8227461253001528\n",
      "  Accuracy on dev  is: 0.8013100436681223\n"
     ]
    }
   ],
   "source": [
    "cls_wv = classify.train_classifier(train_wv,list(sentiment.trainy[0:3107])+list(sentiment.trainy[3108:]))\n",
    "classify.evaluate(train_wv,list(sentiment.trainy[0:3107])+list(sentiment.trainy[3108:]), cls_wv, 'train')\n",
    "classify.evaluate(dev_wv, sentiment.devy, cls_wv, 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dev_wv=[]\n",
    "for i in dev_data:\n",
    "    temp=0;count=0\n",
    "    for j in i:\n",
    "        if j in w2v_model.wv.vocab:\n",
    "            temp+=w2v_model[j]\n",
    "            count+=1\n",
    "    dev_wv.append((temp/count))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "test_wv=[]\n",
    "for i in test_data:\n",
    "    temp=0;count=0\n",
    "    for j in i:\n",
    "        if j in w2v_model.wv.vocab and len(i)!=0:\n",
    "            temp+=w2v_model[j]\n",
    "            count+=1\n",
    "    if count!=0:\n",
    "        test_wv.append((temp/count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pred_kaggle_file(unlabeled, cls, outfname, sentiment):\n",
    "    \"\"\"Writes the predictions in Kaggle format.\n",
    "\n",
    "    Given the unlabeled object, classifier, outputfilename, and the sentiment object,\n",
    "    this function write sthe predictions of the classifier on the unlabeled data and\n",
    "    writes it to the outputfilename. The sentiment object is required to ensure\n",
    "    consistent label names.\n",
    "    \"\"\"\n",
    "    yp = cls.predict(unlabeled)\n",
    "    labels = sentiment.le.inverse_transform(yp)\n",
    "    f = open(outfname, 'w')\n",
    "    f.write(\"ID,LABEL\\n\")\n",
    "    for i in range(len(unlabeled)):\n",
    "        f.write(str(i+1))\n",
    "        f.write(\",\")\n",
    "        f.write(labels[i])\n",
    "        f.write(\"\\n\")\n",
    "    f.close()\n",
    "write_pred_kaggle_file(test_wv, cls_wv, \"sup.csv\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=[]\n",
    "for i in range(len(test_wv)):\n",
    "    if type(test_wv[i])!=np.ndarray or len(test_wv[i])!=300 or type(test_wv[i])==int:\n",
    "        c.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=list(cls_wv.predict(test_wv))\n",
    "for i in range(len(c)):\n",
    "    x.insert(c[i], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "v=pd.read_csv('sentiment-pred.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in x:\n",
    "    if i==1:\n",
    "        y.append('POSITIVE')\n",
    "    else:\n",
    "        y.append('NEGATIVE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "v['LABEL']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "v.to_csv('word2vec',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
